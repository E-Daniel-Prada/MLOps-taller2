# MLOps-Taller2

Este proyecto contiene un entorno de desarrollo para **Machine Learning** utilizando **Docker Compose**.  
El entorno incluye:
âœ… **JupyterLab** para la creaciÃ³n y entrenamiento de modelos.  
âœ… **FastAPI** para exponer los modelos como una API de predicciÃ³n.  
âœ… **Carpeta compartida** entre Jupyter y FastAPI para almacenar los modelos entrenados.  

---

##  **Despliegue del Entorno**  

Para iniciar el entorno:  

```sh
docker compose up --build
```
Esto descargarÃ¡ las imÃ¡genes necesarias y levantarÃ¡ los contenedores de **JupyterLab** y **FastAPI**.  

Para detener los contenedores en cualquier momento, usa:  

```sh
docker compose down
```

Para reiniciar todo desde cero (eliminando volÃºmenes y datos), usa:  

```sh
docker compose down -v
```

---

## ğŸ“‚ **Estructura del Proyecto**  

```
ğŸ“‚ MLOps-Taller2
â”‚â”€â”€ ğŸ“‚ jupyter_work/              # Directorio donde se guardan los notebooks
â”‚â”€â”€ ğŸ“‚ models/                    # Carpeta compartida para almacenar los modelos entrenados
â”‚â”€â”€ ğŸ“‚ app/                        # Carpeta con la API FastAPI
â”‚   â”œâ”€â”€ ğŸ“‚ routes/                 # Endpoints de la API
â”‚   â”‚   â”œâ”€â”€ ğŸ“œ predict.py          # LÃ³gica de predicciÃ³n y carga de modelos
â”‚   â”œâ”€â”€ ğŸ“œ main.py                 # ConfiguraciÃ³n principal de FastAPI
â”‚â”€â”€ ğŸ“œ docker-compose.yml          # ConfiguraciÃ³n de los servicios en Docker
â”‚â”€â”€ ğŸ“œ Dockerfile                  # ConfiguraciÃ³n de la imagen para FastAPI
â”‚â”€â”€ ğŸ“œ requirements.txt            # Dependencias del proyecto
â”‚â”€â”€ ğŸ“œ README.md                   # DocumentaciÃ³n del proyecto
```

---

##  **Acceso a JupyterLab**  

Una vez que el contenedor estÃ¡ en ejecuciÃ³n, acceder a **JupyterLab** en:  

ğŸ”— **URL de acceso:**  
```
http://localhost:8888
```
El acceso estÃ¡ configurado para **no requerir token ni contraseÃ±a**.  

---

##  **Uso de la API de PredicciÃ³n (FastAPI)**  

La API permite consultar los modelos disponibles y realizar predicciones.  

ğŸ“Œ **Swagger UI (DocumentaciÃ³n de la API)**  
Una vez que el servicio estÃ© corriendo:  
ğŸ”— **http://localhost:8989/docs**

---

### ğŸ“Œ **1ï¸ Consultar los Modelos Disponibles**  
Para ver la lista de modelos cargados en el sistema:

```sh
curl -X GET "http://localhost:8989/predict/models"
```

ğŸ“Œ **Ejemplo de respuesta:**  
```json
{
  "1": "app/models/LogisticRegression_optimized.pkl",
  "2": "app/models/DecisionTree_optimized.pkl",
  "3": "app/models/RandomForest_optimized.pkl",
  "4": "models/Jupyter_LogisticRegression_optimized.pkl",
  "5": "models/Jupyter_DecisionTree_optimized.pkl"
}
```
ğŸ”¹ Los modelos en `app/models/` son los pre-cargados.  
ğŸ”¹ Los modelos en `models/` son los generados en **JupyterLab**.  

---

### ğŸ“Œ **2ï¸âƒ£ Realizar una PredicciÃ³n con un Modelo EspecÃ­fico**  

Para hacer una predicciÃ³n, usa el ID de un modelo y envÃ­a los datos de entrada como JSON.  

ğŸ”¹ **Ejemplo:**  
Si queremos usar el modelo **con ID `4`**, enviamos una solicitud con los datos:  
```sh
curl -X POST "http://localhost:8989/predict/4" -H "Content-Type: application/json" -d "[[47.2, 13.7, 214.0, 4925.0, 1, 2]]"
```

ğŸ“Œ **Ejemplo de Respuesta esperada:**  
```json
{
  "model_id": 4,
  "model_path": "models/Jupyter_DecisionTree_optimized.pkl",
  "prediction": [
    2
  ]
}
```
âœ… Esto indica que el modelo fue correctamente cargado y realizÃ³ la predicciÃ³n.  

---


2ï¸âƒ£ **FastAPI detecta automÃ¡ticamente los modelos de `models/` y `app/models/`**  
ğŸ”¹ La API carga modelos desde **ambos directorios** y permite seleccionarlos por ID.  

3ï¸âƒ£ **consultar `/predict/models` para ver los modelos disponibles**  
ğŸ”¹ Luego, seleccionas el ID del modelo para hacer una predicciÃ³n.  

---

##  **ConclusiÃ³n**  

 **Este entorno permite:**  
âœ… Entrenar modelos en **JupyterLab**.  
âœ… Compartir los modelos automÃ¡ticamente con **FastAPI**.  
âœ… Realizar predicciones mediante una API.  
